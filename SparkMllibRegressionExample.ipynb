{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "cb77ea71-bc31-4c91-8795-252579ae8e21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install mlflow=='3.6.0'\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04e11a5d-4591-46d7-bed0-357088d0b174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from mlflow.models.signature import infer_signature\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import mlflow.pyfunc\n",
    "import mlflow.spark\n",
    "import numpy as np\n",
    "\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "import os\n",
    "from mlflow.models import infer_signature\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa5a758a-b80d-402b-a843-4057ece1ea1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a6ec65a-37aa-4bf7-9e84-2662ec7de4a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"DISABLE_MLFLOWDBFS\"] = \"True\"\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f20ce38-6833-4fb6-a32a-f365c5b8d3b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG_NAME = 'pedroz_catalog'\n",
    "SCHEMA_NAME = 'mlops_schema'\n",
    "FEATURE_TABLE_NAME = 'linear_regression_data'\n",
    "MODEL_NAME = 'mllib_linear_regression'\n",
    "TEMP_DIR_VOLUME_NAME = 'temp_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bc78189-760a-4e88-9412-1caba287fbe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create synthetic dataset\n",
    "\n",
    "# Define coefficients for linear relationship\n",
    "coefficients = [2.5, -1.8, 3.2, 0.5, -4.1]\n",
    "intercept = 10.0\n",
    "\n",
    "# Generate 50M rows with 5 features\n",
    "df = spark.range(0, 50_000_000).select(\n",
    "    F.rand(seed=42).alias(\"feature1\"),\n",
    "    F.randn(seed=42).alias(\"feature2\"),\n",
    "    F.rand(seed=24).alias(\"feature3\"),\n",
    "    F.randn(seed=24).alias(\"feature4\"),\n",
    "    F.rand(seed=123).alias(\"feature5\")\n",
    ").withColumn(\n",
    "    \"target\",\n",
    "    coefficients[0]*F.col(\"feature1\") + \n",
    "    coefficients[1]*F.col(\"feature2\") +\n",
    "    coefficients[2]*F.col(\"feature3\") +\n",
    "    coefficients[3]*F.col(\"feature4\") +\n",
    "    coefficients[4]*F.col(\"feature5\") +\n",
    "    intercept + F.randn(seed=42)*0.1\n",
    ")\n",
    "\n",
    "# Save to Unity Catalog\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{CATALOG_NAME}.{SCHEMA_NAME}.{FEATURE_TABLE_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f61f19ac-7025-439a-af4e-23062df4fc18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split train and test datasets\n",
    "\n",
    "train_df, test_df = spark.table(f\"{CATALOG_NAME}.{SCHEMA_NAME}.{FEATURE_TABLE_NAME}\").randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de6f39d5-f5a2-4c31-b150-e1f7bc63ec41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "experiment_name = f\"/Users/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/{MODEL_NAME}_{CATALOG_NAME}\"\n",
    "\n",
    "# Create an MLFlow experiment\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36d98204-1b8c-4d3b-b367-cd46484d224e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "\n",
    "  # Train model\n",
    "  vec_assembler = VectorAssembler(\n",
    "      inputCols=[\"feature1\", \"feature2\", \"feature3\", \"feature4\", \"feature5\"],\n",
    "      outputCol=\"features\"\n",
    "  )\n",
    "  lr = LinearRegression(featuresCol=\"features\", labelCol=\"target\")\n",
    "  pipeline = Pipeline(stages=[vec_assembler, lr])\n",
    "  model = pipeline.fit(train_df)\n",
    "\n",
    "  # Get model signature\n",
    "  X_example = train_df.limit(10).toPandas()[[\"feature1\", \"feature2\", \"feature3\", \"feature4\", \"feature5\"]]\n",
    "  y_example = model.transform(train_df.limit(10)).toPandas()[[\"prediction\"]]\n",
    "  signature = infer_signature(X_example, y_example)\n",
    "\n",
    "  #log model\n",
    "  mlflow.spark.log_model(\n",
    "      spark_model=model,\n",
    "      artifact_path=\"model\",\n",
    "      registered_model_name=f\"{CATALOG_NAME}.{SCHEMA_NAME}.{SCHEMA_NAME}\",\n",
    "      signature=signature,\n",
    "      dfs_tmpdir=f\"/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/{TEMP_DIR_VOLUME_NAME}\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "896ea078-2ad4-4621-9230-d8ed6c43423c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inference comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fbe00ee-2aa2-4e08-b07e-e8ab1ce448ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load input data\n",
    "features_df = spark.table(\"pedroz_catalog.mlops_schema.linear_regression_data\")\n",
    "input_features = features_df.select(\"feature1\", \"feature2\", \"feature3\", \"feature4\", \"feature5\")\n",
    "input_features.limit(5).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4de1e39-f9fb-4811-89e6-e6a92a37e0ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the model_uri for the latest model\n",
    "client = MlflowClient()\n",
    "model_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{MODEL_NAME}\"\n",
    "\n",
    "# Get latest version (you can also replace this by getting a model by its alias, e.g. \"champion\" model)\n",
    "latest_version = max(\n",
    "    [int(mv.version) for mv in client.search_model_versions(f\"name='{model_name}'\")]\n",
    ")\n",
    "print(f\"The latest registered version of model {model_name} is: {latest_version}\")\n",
    "\n",
    "# Define the model_uri\n",
    "model_uri = f'models:/{model_name}/{latest_version}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1abe16ce-cae6-4ecc-a4bd-177d2665723b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Approach 1: load model and transform (recommended for Spark MLlib, since transform already runs distributed inferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec5b6c3c-73b2-471c-be00-019eb860cbe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Load registered model\n",
    "loaded_model = mlflow.spark.load_model(model_uri, dfs_tmpdir=f'/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/{TEMP_DIR_VOLUME_NAME}')\n",
    "\n",
    "# Use .transform() to run predictions\n",
    "loaded_model.transform(input_features).write.mode(\"overwrite\").saveAsTable(f\"{CATALOG_NAME}.{SCHEMA_NAME}.output_table_transform\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print('Inference took %.2f seconds' % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1040588-a2ef-4e21-81dc-e4d327973a57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Approach 2: PyFunc UDF (can be advantageous over regular load_model + predict for other frameworks, since inferences are distributed across all worked nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1af9732-d3bb-4d06-be94-ab382f2f1144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Create UDF\n",
    "loaded_model_udf = mlflow.pyfunc.spark_udf(\n",
    "    spark,\n",
    "    model_uri\n",
    ")\n",
    "\n",
    "input_features.withColumn(\"prediction\", loaded_model_udf()).write.mode(\"overwrite\").saveAsTable(f\"{CATALOG_NAME}.{SCHEMA_NAME}.output_table_udf\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print('Running the predictions with a UDF took %.2f seconds' % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "977a6781-7c3f-4060-8b4d-50ca381a43dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For Spark MLlib, in particular, running inferences using the PyFunc UDF approach is not advantageous, because Spark MLlib natively runs the inferences in a distributed manner, and when you run the PyFunc UDF approach, you first need to convert the Spark binaries into a PyFunc to then distribute it. "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "SparkMllibRegressionExample",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
